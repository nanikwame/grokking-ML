# grokking-ML
Key Learnings from the Book Grokking Machine Learning 

8. Probability Naive Bayes :Bayes’ theorem answers the question, “What is the probability of Y given that x occurred?” which is called a conditional probability. As you can imagine, answering this type of question is useful in machine learning, because if we can answer the question, “What is the probability that the label is positive given the features?” we have a classification model. For example, we can build a sentiment analysis model (just like we did in chapter 6) by answering the question, “What is the probability that this sentence is happy given the words that it contains?” However, when we have too many features (in this case, words), the computation of the probability using Bayes’ theorem gets very complicated. This is where the naive Bayes algorithm comes to our rescue. The naive Bayes algorithm uses a slick simplification of this calculation to help us build our desired classification model, called the naive Bayes model. It’s called naive Bayes because to simplify the calculations, we make a slightly naive assumption that is not necessarily true. 
